{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Justin-Qian/ML_Homework_Collection/blob/main/68510_Expression_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIvjJYhtVvbS"
      },
      "source": [
        "# 6.8510 MiniProject 3: Expression Recognition\n",
        "\n",
        "## Due Friday March 21 at 7pm\n",
        "\n",
        "![alt text](https://i.pinimg.com/originals/56/be/63/56be63d62aa5b174bf9daa13a700a8d2.jpg)\n",
        "Figure 1: Actress Scarlet Johansson making various face expressions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPzFgFEuYt-F"
      },
      "source": [
        "## **1\tIntroduction**\n",
        "\n",
        "The goal of this project is to explore the task of expression analysis and emotion classification over two different data sets: 2D images and 3D point clouds. You will implement and compare several neural network architectures, building on what you learned in Mini Project 2. In addition, you’ll get experience with Transfer Learning for creating personalized models. Please start early and ask questions!\n",
        "The data sets you will use in this project are:\n",
        "\n",
        "1) [Kaggle Facial Expression Recognition Challenge](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expressionrecognition-challenge/data)\n",
        "\n",
        "2)\tPersonal face expression images generated by you\n",
        "\n",
        "3)\tStaff generated point clouds from the iPhone X True-Depth Camera Included with this project is iMotions’s “Facial Expression Analysis: The Complete Pocket Guide.” Use this as a reference throughout the project and please go through it before writing any code. It describes the anatomical characteristics of expression and shows examples of each type of expression.\n",
        "\n",
        "The Facial Action Coding System (FACS) is a tool for measuring expressions first published in 1978 by Ekman and Friesen. It is an anatomical system for describing all observable face movement. It breaks down expressions into individual components of muscle movement known as Activation Units. In this section, we will ask you to describe Facial Expressions based on Activation Units from FACS. A complete guide to FACS and AUs can be found on [iMotions blog](https://imotions.com/blog/facial-action-coding-system/) ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK8nMudUedFQ"
      },
      "source": [
        "## **2\tGetting Started**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaqQX84_edNp"
      },
      "source": [
        "### 2.1 Guide to using Colab\n",
        "Colab is a free Jupyter notebook environment by Google that runs entirely in the cloud. It does not require a setup and allows you to run code and see the output immediately (see [here](https://towardsdatascience.com/a-beginners-tutorial-to-jupyter-notebooks-1b2f8705888a) for more info on Jupyter notebooks and [here](https://www.tutorialspoint.com/google_colab/your_first_colab_notebook.htm) for a walkthrough of how Colab works).\n",
        "\n",
        "Colab notebooks are automatically saved on your Google Drive account but can also be saved on a github account. For the purposes of this lab, you only need to know the very basics of using Jupyter notebooks (and by extension Colab).\n",
        "\n",
        "**Coding Interface**\n",
        "*   You will write your code directly into the code blocks in the notebook\n",
        "*   To run your code, you can either press ctrl-enter when inside the cell, or click the run button in the upper left corner of the cell (need to hover over the brackets for it to appear).\n",
        "*   In order for this lab to work correctly, you should run every cell in order (i.e., as you come upon a code cell, even if it's just staff code, please run it).\n",
        "*   If a cell has been run, a number will appear in brackets in the upper left corner where the run button appears. This number helps you track the order of the calls.\n",
        "*   You are welcome to add any new coding blocks you want (by clicking + Code in the top menu) but you cannot import any more libraries than the ones in this project\n",
        "\n",
        "**Uploading files**\n",
        "*   To upload files from your computer into the notebook (will be required later in the project), click the folder icon on the sidebar on the left. The upload button will let you select the file\n",
        "*   File reading works the same as in a normal IDE. You have to specify the path to your file if it is inside a folder vs. in the main file area.\n",
        "*   Everytime Colab is closed (or refreshed), uploaded files are removed and must be re-uploaded.\n",
        "\n",
        "**Saving files**\n",
        "\n",
        "You will be saving some trained models in this mini project to include in your submission. We suggest that each time you save a model, you also download it to your local machine. Saved files are removed if Colab is closed or refreshed, so you may want to download them just in case.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9YhOr63B20s"
      },
      "source": [
        "### 2.2 Colab Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AysdpAUxuhQ4"
      },
      "source": [
        "**IMPORTANT: Ensure you are using a GPU by going into the file menu above: Runtime -> Change Runtime Type -> GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seLlgFqQemMW"
      },
      "source": [
        "#### Importing Data\n",
        "We will be pulling in two datasets for this mini project. The Kaggle data set is located in kaggle fer2013/fer2013.csv. There are 28K training and 3K testing images in the dataset, each composed of a 48x48 square of pixels and labeled with an emotion [0-6] (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral). The iPhone X dataset is located in iPhoneX/faces.py. There are samples of 50 subjects posing with 7 different expressions. Each sample consists of 1220 (x, y, z) points to make up a depth map. We’ll explore this data more in Section 5.\n",
        "\n",
        "The data will be pulled into a folder names mp4_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oVQcgvzrzlP",
        "outputId": "29f8de53-f458-4e42-a428-f597bfcec4c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mp4_data'...\n",
            "remote: Enumerating objects: 22, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 22 (delta 3), reused 22 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (22/22), 93.64 MiB | 18.25 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone  https://gitlab.com/andwong/mp4_data.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0VEuwaGdv9V"
      },
      "source": [
        "#### Setting up imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq05uZUfkUyW"
      },
      "source": [
        "The output will mention package incompatibilities - do not worry about this, it is just to do with some inconsistencies in the version of Tensorflow we are using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcNlTVaGBqPg",
        "outputId": "c52b276f-3558-46f4-d82e-c70c6a05936a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting h5py==2.10.0\n",
            "  Downloading h5py-2.10.0.tar.gz (301 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/301.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.1/301.1 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy>=1.7 (from h5py==2.10.0)\n",
            "  Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting six (from h5py==2.10.0)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: h5py\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for h5py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for h5py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for h5py\n",
            "Failed to build h5py\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (h5py)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install 'h5py==2.10.0' --force-reinstall\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing import image\n",
        "\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, AveragePooling2D\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3bSyF3gd2aZ"
      },
      "source": [
        "#### Predefined Staff Variables/Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ux5GrGrkNy8v"
      },
      "outputs": [],
      "source": [
        "emotions = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
        "\n",
        "def plot_emotion_prediction(pred):\n",
        "    \"\"\"\n",
        "    Plots the prediction for each emotion on a bar chart\n",
        "    :param pred: the predictions for each emotions\n",
        "    \"\"\"\n",
        "    labels = np.arange(len(emotions))\n",
        "    plt.bar(labels, pred, align='center', alpha=0.5)\n",
        "    plt.xticks(labels, emotions)\n",
        "    plt.ylabel('prediction')\n",
        "    plt.title('emotion')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOfQfKAi2W5_"
      },
      "source": [
        "### 2.3 Mini Project Structure\n",
        "\n",
        "In this miniproject, you will\n",
        "\n",
        "1) \tCode a Convolutional Neural Network using Keras to detect Face Expressions from a large dataset of images\n",
        "\n",
        "2)\tUse transfer learning to adapt the CNN you’ve created into a personalized network for your own expressions (you’ll have to supply the data)\n",
        "\n",
        "3)\tExplore a new type of available face data - point clouds from the iPhone X True Depth Camera and classify samples by modifying your original CNN.\n",
        "\n",
        "Please do not hesitate to post for help on Piazza using the miniproject3 tag."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6XWup6RaSC0"
      },
      "source": [
        "## **3\tExpression Recognition with Convolutional Neural Networks**\n",
        "As you read about and saw during the lecture on face recognition, the hidden layers of a Convolutional Neural Network (CNN) typically consist of some combination of convolutional layers, pooling layers, fully connected layers and normalization layers. We’ll give a brief overview of the CNNs and what these layers do. If you don’t have any experience with CNNs, watch [this video](https://www.youtube.com/watch?v=YRhxdVk_sIs) on YouTube to get a solid understanding of how they function.\n",
        "\n",
        "Together, we’ll code up a simple CNN to process the Kaggle dataset. You are encouraged to improve this model by adding additional layers. The Input layer will take in image data (represented as a matrix of numbers) and pass them into a convolution layer. Here the image data is “convolved” - a filter (i.e., a function) is applied methodically to overlapping tiles of the input. The values the filter produces (technically, the dot product of the filter with each sub matrix, is itself another matrix of data. The final layer, the fully connected layer, takes the convolution and produces a vector of predictions.\n",
        "\n",
        "Now it’s time to write a CNN using the Keras API and Tensorflow backend. We’ve already started an implementation for you below. You should complete the implementation by following these steps:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pes_LpVBpy_"
      },
      "source": [
        "###Part A\n",
        "We need to understand our input before we can begin our model. The Kaggle dataset contains 35888 images: 28709 for training and 3589 for testing. Let’s organize this data so we can use it in out model.\n",
        "\n",
        "The supplied code imports the data from the .csv file for you. Each line of data contains an emotion label, image data, and test/train usage. The emotion label is a number between 0 and 6, corresponding to the labels specified above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olVJFVlOyzN9"
      },
      "source": [
        "1) We parse the .csv file for you into x train, the training image pixel data as 1D arrays of pixels, y train, the labels corresponding to the training images, x test, the testing image pixel data as 1D arrays of pixels, y test, the labels corresponding to the test images. You will need to do this on your own later in the mini project\n",
        "\n",
        "2)\tThe pixel values in the image data are strings. Convert them to float32 and normalize the inputs to be between 0 and 1 (where the image data is on the scale [0, 255]).\n",
        "\n",
        "\n",
        "3) Reshape the image data so we can enter samples into our modes with the shape (48, 48, 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvOsTz1xvB6A"
      },
      "outputs": [],
      "source": [
        "num_classes = 7 #angry, disgust, fear, happy, sad, surprise, neutral\n",
        "batch_size = 256\n",
        "epochs = 5\n",
        "\n",
        "def split_data():\n",
        "  # READ IN KAGGLE DATA\n",
        "  with open(\"mp4_data/kaggle_fer2013/fer2013.csv\") as file:\n",
        "      data = file.readlines()\n",
        "\n",
        "  lines = np.array(data)\n",
        "\n",
        "  x_train, y_train, x_test, y_test = [], [], [], []\n",
        "\n",
        "  # A. 1) SPLIT DATA INTO TEST AND TRAIN\n",
        "  for i in range(1,lines.size):\n",
        "      emotion, img, usage = lines[i].split(\",\")\n",
        "      val = img.split(\" \")\n",
        "      pixels = np.array(val, 'float32')\n",
        "      emotion = keras.utils.to_categorical(emotion, num_classes)\n",
        "\n",
        "      if 'Training' in usage:\n",
        "          y_train.append(emotion)\n",
        "          x_train.append(pixels)\n",
        "      elif 'PublicTest' in usage:\n",
        "          y_test.append(emotion)\n",
        "          x_test.append(pixels)\n",
        "\n",
        "  # A. 2) CAST AND NORMALIZE DATA\n",
        "  normalized_train_list = [arr / 255.0 for arr in x_train]\n",
        "  normalized_test_list = [arr / 255.0 for arr in x_test]\n",
        "\n",
        "  # A. 3) RESHAPE DATA\n",
        "  x_train = [arr.reshape(48, 48, 1) for arr in normalized_train_list]\n",
        "  x_test = [arr.reshape(48, 48, 1) for arr in normalized_test_list]\n",
        "\n",
        "  x_train = np.array(x_train)\n",
        "  y_train = np.array(y_train)\n",
        "  x_test = np.array(x_test)\n",
        "  y_test = np.array(y_test)\n",
        "\n",
        "  return x_train, y_train, x_test, y_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xfYhovltrk2"
      },
      "source": [
        "### Part B\n",
        "Now it’s time to code the CNN. We’ll make use of the Keras functional API for building models. For more information on Keras, see the Keras Tutorial online and explore the documentation [here](https://keras.io/getting-started/functional-api-guide/) . We’ll create a simple model together that gives you a working solution. However, we will later expect you to add additional layers to this model to improve its performance.\n",
        "\n",
        "1)\tCreate an Input layer that takes in data of shape (48, 48, 1, ). This is the size of our photos\n",
        "\n",
        "2)\tAdd a Convolutional layer to the network using the 2D convolution layer for spatial convolution over images. Make sure the layer has the following properties: filters=64., kernel_size=(5,5), activation=‘relu’\n",
        "\n",
        "3)\tAdd a MaxPooling2D layer with pool_size=(5,5) and strides=(2, 2)\n",
        "\n",
        "4)\tAdd a Flatten layer which converts the data into a 1D feature vector ready for classification\n",
        "\n",
        "5)\tAdd a Dense layer with 1024 units and activation=‘relu’\n",
        "\n",
        "6)\tAdd a final Dense layer with 7 units (for classification) and the ‘softmax’ activation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMDdYDmBtsbm"
      },
      "outputs": [],
      "source": [
        "# B. CREATE CNN MODEL\n",
        "\n",
        "def create_model():\n",
        "  inputs = Input(shape=(48, 48, 1, ))\n",
        "  # INSERT LAYERS HERE\n",
        "  x = Conv2D(filters=64, kernel_size=(5, 5), activation='relu')(inputs)\n",
        "  x = MaxPooling2D(pool_size=(5, 5), strides=(2, 2))(x)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(1024, activation='relu')(x)\n",
        "  outputs = Dense(7, activation='softmax')(x)\n",
        "\n",
        "  return Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa2syoyZuCvR"
      },
      "source": [
        "### Part C\n",
        "Now it’s time to train our model and see how well it performs.\n",
        "\n",
        "1) Compile your model with loss=‘categorical_crossentropy’ and the Adam optimizer (i.e. keras.optimizers.Adam()).\n",
        "\n",
        "2) Train your model by calling model.fit (...) and the provided steps_per_epoch = batch size and epochs = 5 variables. Ensure that you batch the training and testing data in the Model.fit(..., batch  size = 256) method.\n",
        "\n",
        "3) Make sure to save your model after training it: export the model to a .h5 file using the built in model.save(‘model 1.h5’) (please use this naming convention). The model should take about 10 minutes to run and should achieve about 55% accuracy.\n",
        "\n",
        "**Note: you may want to download the model to your local machine just incase Colab crashes or unexpectedly closes. You can download by right-clicking on the model in the file sidebar and selecting download**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zNiXpSckCf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd958195-2904-47ae-b76b-5d7adce99202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m113/256\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.2332 - loss: 2.4561"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.2614 - loss: 2.1644 - val_accuracy: 0.3842 - val_loss: 1.6129\n",
            "Epoch 2/5\n",
            "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.4089 - loss: 1.5467 - val_accuracy: 0.4361 - val_loss: 1.4943\n",
            "Epoch 3/5\n",
            "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.4603 - loss: 1.4225 - val_accuracy: 0.4737 - val_loss: 1.4018\n",
            "Epoch 4/5\n",
            "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.4883 - loss: 1.3462 - val_accuracy: 0.4553 - val_loss: 1.4131\n",
            "Epoch 5/5\n",
            "\u001b[1m256/256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.5213 - loss: 1.2701 - val_accuracy: 0.4787 - val_loss: 1.3674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "num_classes = 7 #angry, disgust, fear, happy, sad, surprise, neutral\n",
        "batch_size = steps_per_epoch = 256\n",
        "epochs = 5\n",
        "\n",
        "def cnn():\n",
        "  x_train, y_train, x_test, y_test = split_data()\n",
        "  model = create_model()\n",
        "\n",
        "  # C. 1) COMPILE MODEL\n",
        "  model.compile(\n",
        "        optimizer=keras.optimizers.Adam(),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "  # C. 2) DATA BATCH; TRAIN AND SAVE MODEL\n",
        "  model.fit(\n",
        "        x_train, y_train,\n",
        "        validation_data=(x_test, y_test),\n",
        "        batch_size=batch_size,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        epochs=epochs\n",
        "    )\n",
        "\n",
        "  model.save('model_1.h5')\n",
        "\n",
        "cnn()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_a-5wUIassw"
      },
      "source": [
        "4) You can test your model with the script below to see how a certain example (jackman.png) gets classified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rslpj5PB2Wdu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "2eef044c-e69c-40cd-9fc2-0622c019c480"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 546ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def test_cnn():\n",
        "  model = load_model('model_1.h5')\n",
        "\n",
        "  img = image.load_img(\"mp4_data/jackman.png\", color_mode = \"grayscale\", target_size=(48, 48))\n",
        "\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis = 0)\n",
        "\n",
        "  x /= 255\n",
        "\n",
        "  custom = model.predict(x)\n",
        "  plot_emotion_prediction(custom[0])\n",
        "\n",
        "test_cnn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsrwcrMCmJeB"
      },
      "source": [
        "### Part D\n",
        "\n",
        "Now, modify the model to improve your accuracy. You may change the parameters (such as batch size) and layers of the model. We've provided new code blocks below for you to experiment in (you can copy over your code from part C).\n",
        "\n",
        " In your writeup, include:\n",
        "\n",
        "*   A diagram of your final network architecture\n",
        "*   A description of the structure and the parameters you used.\n",
        "*   The accuracy and loss for your model\n",
        "\n",
        "In the original Kaggle challenge, the winner achieved just 34% accuracy - so congrats, your model is already much better! Be sure to save your trained model as model_2.h5, i.e., following our naming convention.\n",
        "\n",
        "**Ensure you are using a GPU by going into the file menu above: Runtime -> Change Runtime Type -> GPU (not TPU). If it was not selected, re-run all the code up to this point.**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "05tQcWeTcGtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-J3qUpc4PRd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "outputId": "82fd1305-d13a-404c-bdee-a44ee28f516e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - accuracy: 0.2930 - loss: 1.7581 - val_accuracy: 0.4416 - val_loss: 1.4562\n",
            "Epoch 2/20\n",
            "\u001b[1m  1/448\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14:02\u001b[0m 2s/step - accuracy: 0.3514 - loss: 1.7240"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 631us/step - accuracy: 0.3514 - loss: 1.7240 - val_accuracy: 0.4452 - val_loss: 1.4522\n",
            "Epoch 3/20\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 12ms/step - accuracy: 0.4467 - loss: 1.4433 - val_accuracy: 0.4937 - val_loss: 1.3380\n",
            "Epoch 4/20\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 512us/step - accuracy: 0.4054 - loss: 1.5582 - val_accuracy: 0.4962 - val_loss: 1.3385\n",
            "Epoch 5/20\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.5055 - loss: 1.3035 - val_accuracy: 0.4987 - val_loss: 1.2881\n",
            "Epoch 6/20\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - accuracy: 0.4595 - loss: 1.1151 - val_accuracy: 0.4979 - val_loss: 1.2890\n",
            "Epoch 7/20\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.5359 - loss: 1.2221 - val_accuracy: 0.5213 - val_loss: 1.2494\n",
            "Epoch 8/20\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - accuracy: 0.5946 - loss: 1.0694 - val_accuracy: 0.5199 - val_loss: 1.2477\n",
            "Epoch 9/20\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.5738 - loss: 1.1300 - val_accuracy: 0.5305 - val_loss: 1.2380\n",
            "Epoch 10/20\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621us/step - accuracy: 0.5676 - loss: 1.1100 - val_accuracy: 0.5300 - val_loss: 1.2348\n",
            "Epoch 11/20\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.6115 - loss: 1.0447 - val_accuracy: 0.5439 - val_loss: 1.2191\n",
            "Epoch 12/20\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - accuracy: 0.6486 - loss: 1.0312 - val_accuracy: 0.5439 - val_loss: 1.2217\n",
            "Epoch 13/20\n",
            "\u001b[1m173/448\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.6588 - loss: 0.9288"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-38cf2c06bed4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_2.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-38cf2c06bed4>\u001b[0m in \u001b[0;36mcnn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[0;31m# C. 2) DATA BATCH; TRAIN AND SAVE MODEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m   history = model.fit(\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[1;32m    219\u001b[0m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/optional_ops.py\u001b[0m in \u001b[0;36mhas_value\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    174\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m       return gen_optional_ops.optional_has_value(\n\u001b[0m\u001b[1;32m    177\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_optional_ops.py\u001b[0m in \u001b[0;36moptional_has_value\u001b[0;34m(optional, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m    173\u001b[0m         _ctx, \"OptionalHasValue\", name, optional)\n\u001b[1;32m    174\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "num_classes = 7 #angry, disgust, fear, happy, sad, surprise, neutral\n",
        "batch_size = 64\n",
        "steps_per_epoch = 28709 // batch_size\n",
        "epochs = 20\n",
        "\n",
        "def create_model():\n",
        "  inputs = Input(shape=(48, 48, 1, ))\n",
        "  # INSERT LAYERS HERE\n",
        "  # Layer1\n",
        "  x = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
        "  x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "  # Layer2\n",
        "  x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "  x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(512, activation='relu')(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "\n",
        "  outputs = Dense(7, activation='softmax')(x)\n",
        "\n",
        "  return Model(inputs, outputs)\n",
        "\n",
        "def cnn():\n",
        "  x_train, y_train, x_test, y_test = split_data()\n",
        "  model = create_model()\n",
        "\n",
        "  # Add an early stopping mechanism\n",
        "  early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        "  )\n",
        "\n",
        "  # C. 1) COMPILE MODEL\n",
        "  model.compile(\n",
        "        optimizer=keras.optimizers.Adam(),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "  # C. 2) DATA BATCH; TRAIN AND SAVE MODEL\n",
        "  history = model.fit(\n",
        "        x_train, y_train,\n",
        "        validation_data=(x_test, y_test),\n",
        "        batch_size=batch_size,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        epochs=epochs,\n",
        "        callbacks=[early_stop]\n",
        "    )\n",
        "\n",
        "  best_epoch = np.argmin(history.history['val_loss']) + 1\n",
        "  best_val_acc = history.history['val_accuracy'][best_epoch - 1]\n",
        "  print(f\"✨ Best epoch: {best_epoch}\")\n",
        "  print(f\"✅ Validation accuracy at best epoch: {best_val_acc:.4f}\")\n",
        "\n",
        "  model.save('model_2.h5')\n",
        "\n",
        "cnn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Qfv0PTD4lQA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "8fad7dd1-e324-4959-c8e1-233fc9db845d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 592ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def test_cnn():\n",
        "  model = load_model('model_2.h5')\n",
        "\n",
        "  img = image.load_img(\"mp4_data/jackman.png\", color_mode = \"grayscale\", target_size=(48, 48))\n",
        "\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis = 0)\n",
        "\n",
        "  x /= 255\n",
        "\n",
        "  custom = model.predict(x)\n",
        "  plot_emotion_prediction(custom[0])\n",
        "\n",
        "test_cnn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7V-cFtlbwZ8"
      },
      "source": [
        "##4\tTransfer Learning for a Personalized Machine Learning Model\n",
        "\n",
        "In practice, very few people train an entire Convolutional Network from scratch (with random initialization) as we just did. This is because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pre-train a CNN on a (different) very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the CNN either as an initialization or a fixed feature extractor for the task of interest.\n",
        "\n",
        "The process of sharing results across different problems is known as Transfer Learning. In other words, Transfer Learning is a machine learning technique where a model trained on one task is re-purposed for a second related task.\n",
        "\n",
        "In the section, we’ll first ask you to generate your own expression data (yes, you’ll have to take pictures of yourself) and use that data to fine-tune your CNN from the previous section into a personalized model for your face, using Transfer Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emQCJU-0RNuu"
      },
      "source": [
        "###Part E\n",
        "Pose in front of a camera or get a friend to help you take 7 pictures: one for each type of expression: angry, disgust, fear, happy, sad, surprise, neutral. This is your training data. Crop these images so that they contain a bounding box of only your face. It is OK if the pictures are not 48 x 48, Keras will resize them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO1AgNsxRP_y"
      },
      "source": [
        "###Part F\n",
        "Take 7 more pictures. This is your testing data.\n",
        "\n",
        "Add your training and testing images to the notebook:\n",
        "\n",
        "1) Open the files section of the sidebar on the left side of your screen\n",
        "\n",
        "2) Right-click in the area to open the context menu. Select create new folder.\n",
        "\n",
        "3) Upload your images (you can upload multiple at a time)\n",
        "\n",
        "4) Structure the folder and name your images as you like"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdLjc4zLRSGp"
      },
      "source": [
        "###Part G\n",
        "Now it’s time to load your model from the previous section. We’ve started an implementation for you below.\n",
        "\n",
        "1) Import the data from your images and reshape the data so that you can retrain your model from Section 3 (model 2.h5). You will need to grayscale your images. You may find functions in keras.preprocessing useful for image manipulation.\n",
        "\n",
        "2) Load your model using load_model(‘model_2.h5’) and train the model on your 7 training images.\n",
        "\n",
        "3) Finally, test the newly trained model on your test images. Save your trained model as model_3.h5. Remember we provided the helper function plot_emotion_prediction(pred) at the beginning of this notebook that takes in a model prediction values from a single call of model.predict(x) and plots them on a bar graph."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"images.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"images\")"
      ],
      "metadata": {
        "id": "C1umPL4_oVIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import os"
      ],
      "metadata": {
        "id": "KDM1VZcAops6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_cnn(model_path, img_path):\n",
        "  model = load_model(model_path)\n",
        "\n",
        "  img = image.load_img(img_path, color_mode = \"grayscale\", target_size=(48, 48))\n",
        "\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis = 0)\n",
        "\n",
        "  x /= 255\n",
        "\n",
        "  custom = model.predict(x)\n",
        "  plot_emotion_prediction(custom[0])"
      ],
      "metadata": {
        "id": "lr0YDr7erlbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLRuqqw1mB4g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6b1c6d22-76c3-40bb-bdcc-939094624cea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 523ms/step - accuracy: 0.1405 - loss: 4.0554\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.1976 - loss: 4.1343    \n",
            "Epoch 3/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.1976 - loss: 3.2150    \n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.1405 - loss: 2.3363    \n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.1476 - loss: 2.4649    \n",
            "Epoch 6/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.2810 - loss: 2.8890    \n",
            "Epoch 7/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.1476 - loss: 2.4362    \n",
            "Epoch 8/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.2976 - loss: 2.4821\n",
            "Epoch 9/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2310 - loss: 2.4909    \n",
            "Epoch 10/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6119 - loss: 1.1815\n",
            "Epoch 11/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5786 - loss: 1.5717\n",
            "Epoch 12/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.0571 - loss: 1.7070    \n",
            "Epoch 13/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3881 - loss: 1.8428\n",
            "Epoch 14/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6190 - loss: 1.6512\n",
            "Epoch 15/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8524 - loss: 1.1514\n",
            "Epoch 16/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5286 - loss: 1.5805\n",
            "Epoch 17/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6286 - loss: 1.2538\n",
            "Epoch 18/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6119 - loss: 1.5178\n",
            "Epoch 19/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6190 - loss: 1.2298\n",
            "Epoch 20/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8024 - loss: 0.7141\n",
            "Epoch 21/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5286 - loss: 1.2956\n",
            "Epoch 22/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8024 - loss: 0.7631\n",
            "Epoch 23/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6786 - loss: 1.1827\n",
            "Epoch 24/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4714 - loss: 1.1030\n",
            "Epoch 25/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2405 - loss: 1.1634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "num_classes = 7 #angry, disgust, fear, happy, sad, surprise, neutral\n",
        "batch_size = 2\n",
        "epochs = 50\n",
        "\n",
        "def transfer():\n",
        "  # G. 1) Import data and reshape data\n",
        "  train_path = \"images/train\"\n",
        "  test_path = \"images/test\"\n",
        "\n",
        "  img_size = (48, 48)\n",
        "\n",
        "  def load_images_from_folder(folder):\n",
        "    x = []\n",
        "    y = []\n",
        "    for label in range(num_classes):\n",
        "        img_path = os.path.join(folder, f\"{label}.jpg\")\n",
        "        img = load_img(img_path, color_mode=\"grayscale\", target_size=img_size)\n",
        "        img_array = img_to_array(img) / 255.0\n",
        "        x.append(img_array)\n",
        "        y.append(label)\n",
        "    return np.array(x), to_categorical(y, num_classes)\n",
        "\n",
        "  x_train, y_train = load_images_from_folder(train_path)\n",
        "  x_test, y_test = load_images_from_folder(test_path) #test folder is not organized\n",
        "\n",
        "  # G. 2) Load model and train on training images\n",
        "  model = load_model('model_2.h5')\n",
        "\n",
        "  for layer in model.layers[:2]:\n",
        "    layer.trainable = False\n",
        "\n",
        "  model.compile(optimizer=keras.optimizers.Adam(1e-4),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "  early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "  history = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          callbacks=[early_stop])\n",
        "\n",
        "  model.save(\"model_3.h5\")\n",
        "\n",
        "  # G. 3) Test newly trained model and save\n",
        "  model_path = \"model_3.h5\"\n",
        "  img_path = \"images/test/0.jpg\"\n",
        "  test_cnn(model_path, img_path)\n",
        "\n",
        "transfer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compare():\n",
        "    test_path = \"images/test\"\n",
        "    img_size = (48, 48)\n",
        "\n",
        "    model_before = load_model('model_2.h5')\n",
        "    model_after = load_model('model_3.h5')\n",
        "\n",
        "    print(f\"{'Label':<6}{'Before':<10}{'After':<10}{'Match Before':<15}{'Match After'}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for i in range(7):  # 0~6\n",
        "        img_file = os.path.join(test_path, f\"{i}.jpg\")\n",
        "        img = load_img(img_file, color_mode=\"grayscale\", target_size=img_size)\n",
        "        x = img_to_array(img) / 255.0\n",
        "        x = np.expand_dims(x, axis=0)\n",
        "\n",
        "        pred_before = model_before.predict(x, verbose=0)\n",
        "        pred_after = model_after.predict(x, verbose=0)\n",
        "\n",
        "        pred_before_label = np.argmax(pred_before[0])\n",
        "        pred_after_label = np.argmax(pred_after[0])\n",
        "\n",
        "        print(f\"{i:<6}{emotions[pred_before_label]:<10}{emotions[pred_after_label]:<10}{str(pred_before_label==i):<15}{str(pred_after_label==i)}\")\n",
        "\n",
        "compare()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfCcRgtTuqad",
        "outputId": "4306bf86-e9de-481e-dcb0-c7e206c6ed8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Before    After     Match Before   Match After\n",
            "--------------------------------------------------\n",
            "0     happy     angry     False          True\n",
            "1     happy     happy     False          False\n",
            "2     neutral   sad       False          False\n",
            "3     happy     happy     True           True\n",
            "4     happy     sad       False          True\n",
            "5     neutral   sad       False          False\n",
            "6     neutral   sad       True           False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8iIYdQHAyOb"
      },
      "source": [
        "4)  Which expressions are not being recognized? Why do you think some expressions are recognized better than others? Report the accuracy of the model.\n",
        "\n",
        "5) If your model did not achieve good accuracy on your personal data, explain why you think that is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCvEX_msnO3J"
      },
      "source": [
        "## 5\tFrom Images to Depth: Next Generation of Face Representation\n",
        "In this section, we expect you to implement a fully functional model on your own. Your work in this part will be graded on correctness, not on how accurate your final model is.\n",
        "\n",
        "Recall structured light: the technology used by the Xbox Kinect that shines thousands of infrared dots in an area and can create a corresponding depth map. The iPhone X also uses this technology to scan a user’s face. We’ve accumulated samples of 100 iPhone X users posing with different face expressions using [Apple’s ARKit framework](https://developer.apple.com/documentation/arkit/arfaceanchor?language=objc) . Apple anchors the face into a specific origin, and provides us with vertex positions for each point in the face mesh. These vertices are referred to as a point cloud. The point cloud we receive is sparse, so we see a smoothed version of an actual face. Together, this normalizes all of our data and makes it ready for analysis.\n",
        "\n",
        "![alt text](https://www.bing.com/th?id=OIP.nxQY5PjyWSCBU1oYGGqy3AHaDS&pid=Api&rs=1)\n",
        "\n",
        "Figure 2: Origin of the face coordinate system.\n",
        "\n",
        "\n",
        "In this last part, we’ll classify the 3D data into the same 7 emotions: angry, disgust, fear, happy, sad, surprise, neutral. However, this time we’re expecting you to craft the model. The model you will create should be a 3D Convolutional Neural Network. They are the same in essence to 2D CNNs, but perform operations in 3 dimensions. In order to use a 3D CNN, we’ll have to transform our input data into voxels. Voxels are the three-dimensional analogue of a pixel:\n",
        "unit volumes of space that contain a value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB7Tu2yxnb2_"
      },
      "source": [
        "\n",
        "### Part H\n",
        "To visualize the various expressions, you will need to use the Visualization GUI we provided in the zip file for you to run on your local machine. To use the GUI, run\n",
        "\n",
        "```\n",
        "python show_gui.py\n",
        "```\n",
        "\n",
        "\n",
        "You’ll be able to see the 7 different expressions. You can drag the graph to view the data from different orientations.\n",
        "\n",
        "\n",
        "\n",
        "1) Inspect the 3D face data and give us your impressions. Compare the expression data using FACS. Which expressions are the most unique? Which expressions are most similar? What information does the point cloud provide us that the image does not?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N94tERYLiiZL"
      },
      "source": [
        "Now it's time to try your hand at making your own model here in Colab! The data is provided as a Python dictionary\n",
        "\n",
        "```\n",
        "face_samples = { sample id : { emotion: { x: [...], y: [...], z: [...]}}}\n",
        "```\n",
        "\n",
        "2)  Read in the iPhone X data. For each point cloud, create a 24 x 24 x 24 voxel grid represented as a 3D numpy array initialized with all 0s. For each point in the cloud, increment the value of the voxel that the point falls in.\n",
        "\n",
        "3) Construct a 3D Convolution Neural Network using Keras. You can use your previous work as a starting point, but will have to make use of Conv3D and MaxPool3D from Keras. You are free to add as many layers as you’d like.\n",
        "\n",
        "4) Train and test your model. Save your trained model as model_4.h5.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# H. 2) READ IN iPHONE X DATA AND SHAPE\n",
        "%run mp4_data/iPhoneX/faces.py"
      ],
      "metadata": {
        "id": "yFyY7QYRjtr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotions = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
        "emotion_to_label = {emo: i for i, emo in enumerate(emotions)}\n",
        "\n",
        "def pointcloud_to_voxel(emotion_cloud, grid_size=24):\n",
        "    x = np.array(emotion_cloud['x'])\n",
        "    y = np.array(emotion_cloud['y'])\n",
        "    z = np.array(emotion_cloud['z'])\n",
        "\n",
        "    # Normalized\n",
        "    x_norm = (x - x.min()) / (x.max() - x.min() + 1e-8)\n",
        "    y_norm = (y - y.min()) / (y.max() - y.min() + 1e-8)\n",
        "    z_norm = (z - z.min()) / (z.max() - z.min() + 1e-8)\n",
        "\n",
        "    # Mapping\n",
        "    xi = np.clip((x_norm * (grid_size - 1)).astype(int), 0, grid_size - 1)\n",
        "    yi = np.clip((y_norm * (grid_size - 1)).astype(int), 0, grid_size - 1)\n",
        "    zi = np.clip((z_norm * (grid_size - 1)).astype(int), 0, grid_size - 1)\n",
        "\n",
        "    # Initialization\n",
        "    voxel_grid = np.zeros((grid_size, grid_size, grid_size), dtype=np.int32)\n",
        "\n",
        "    # Incrementation\n",
        "    for i in range(len(xi)):\n",
        "        voxel_grid[xi[i], yi[i], zi[i]] += 1\n",
        "\n",
        "    return voxel_grid\n",
        "\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for sample_id, emotions in face_samples.items():\n",
        "    for emotion_name, cloud in emotions.items():\n",
        "        voxel = pointcloud_to_voxel(cloud, grid_size=24)\n",
        "        x.append(voxel)\n",
        "        y.append(emotion_to_label[emotion_name])"
      ],
      "metadata": {
        "id": "TH266xfbhNeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array(x).reshape(-1, 24, 24, 24, 1)\n",
        "y = keras.utils.to_categorical(y, num_classes=7)\n",
        "\n",
        "print(\"x shape:\", x.shape)  # (N, 24, 24, 24, 1)\n",
        "print(\"y shape:\", y.shape)  # (N, 7)\n",
        "print(\"y[0] =\", y[0])       # 如：[0. 0. 0. 1. 0. 0. 0.] -> happy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR3bIfSVj5ml",
        "outputId": "d91adfaf-6e00-464c-f472-fcd9c3dbffe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x shape: (49, 24, 24, 24, 1)\n",
            "y shape: (49, 7)\n",
            "y[0] = [1. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "dbw4Z016lOh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# H. 3) CREATE MODEL OF CHOICE\n",
        "def create_model_3d():\n",
        "    inputs = Input(shape=(24, 24, 24, 1))  # 和你的 x 数据 shape 对应\n",
        "\n",
        "    # Layer 1\n",
        "    x = Conv3D(32, kernel_size=3, activation='relu', padding='same')(inputs)\n",
        "    x = MaxPooling3D(pool_size=2)(x)  # 输出 -> (12, 12, 12, 32)\n",
        "\n",
        "    # Layer 2\n",
        "    x = Conv3D(64, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = MaxPooling3D(pool_size=2)(x)  # 输出 -> (6, 6, 6, 64)\n",
        "\n",
        "    # Layer 3\n",
        "    x = Conv3D(128, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = MaxPooling3D(pool_size=2)(x)  # 输出 -> (3, 3, 3, 128)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "\n",
        "    outputs = Dense(7, activation='softmax')(x)\n",
        "\n",
        "    return Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "Bo2JxVhGlb30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPduEIeGiC-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa0784b8-b6db-4012-9ee7-fa4a01db2032"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 597ms/step - accuracy: 0.1049 - loss: 2.0661 - val_accuracy: 0.5000 - val_loss: 1.8810\n",
            "Epoch 2/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.2368 - loss: 1.9060 - val_accuracy: 0.2000 - val_loss: 1.8916\n",
            "Epoch 3/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.2442 - loss: 1.8978 - val_accuracy: 0.5000 - val_loss: 1.8510\n",
            "Epoch 4/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.2090 - loss: 1.8422 - val_accuracy: 0.3000 - val_loss: 1.8065\n",
            "Epoch 5/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.3761 - loss: 1.7918 - val_accuracy: 0.4000 - val_loss: 1.7771\n",
            "Epoch 6/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3872 - loss: 1.7206 - val_accuracy: 0.3000 - val_loss: 1.7529\n",
            "Epoch 7/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4419 - loss: 1.7292 - val_accuracy: 0.5000 - val_loss: 1.7327\n",
            "Epoch 8/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.4579 - loss: 1.6666 - val_accuracy: 0.5000 - val_loss: 1.7097\n",
            "Epoch 9/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.3650 - loss: 1.6306 - val_accuracy: 0.7000 - val_loss: 1.6855\n",
            "Epoch 10/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.4763 - loss: 1.6456 - val_accuracy: 0.6000 - val_loss: 1.6531\n",
            "Epoch 11/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.5060 - loss: 1.5535 - val_accuracy: 0.6000 - val_loss: 1.6097\n",
            "Epoch 12/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.4624 - loss: 1.4928 - val_accuracy: 0.7000 - val_loss: 1.5710\n",
            "Epoch 13/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5013 - loss: 1.4323 - val_accuracy: 0.6000 - val_loss: 1.5281\n",
            "Epoch 14/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5404 - loss: 1.4300 - val_accuracy: 0.6000 - val_loss: 1.5102\n",
            "Epoch 15/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.6647 - loss: 1.3261 - val_accuracy: 0.7000 - val_loss: 1.5069\n",
            "Epoch 16/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4652 - loss: 1.4344 - val_accuracy: 0.6000 - val_loss: 1.4882\n",
            "Epoch 17/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.4365 - loss: 1.5022 - val_accuracy: 0.6000 - val_loss: 1.4597\n",
            "Epoch 18/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.6444 - loss: 1.2931 - val_accuracy: 0.8000 - val_loss: 1.4260\n",
            "Epoch 19/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7122 - loss: 1.3407 - val_accuracy: 0.8000 - val_loss: 1.3860\n",
            "Epoch 20/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7622 - loss: 1.2051 - val_accuracy: 0.8000 - val_loss: 1.3536\n",
            "Epoch 21/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6769 - loss: 1.2923 - val_accuracy: 0.9000 - val_loss: 1.3305\n",
            "Epoch 22/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.6936 - loss: 1.2550 - val_accuracy: 0.6000 - val_loss: 1.3104\n",
            "Epoch 23/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6073 - loss: 1.2704 - val_accuracy: 0.6000 - val_loss: 1.3251\n",
            "Epoch 24/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7233 - loss: 1.2346 - val_accuracy: 0.6000 - val_loss: 1.3061\n",
            "Epoch 25/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6509 - loss: 1.1619 - val_accuracy: 0.6000 - val_loss: 1.2591\n",
            "Epoch 26/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.6462 - loss: 1.1307 - val_accuracy: 0.6000 - val_loss: 1.2248\n",
            "Epoch 27/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5842 - loss: 1.1922 - val_accuracy: 0.6000 - val_loss: 1.2225\n",
            "Epoch 28/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6175 - loss: 1.1867 - val_accuracy: 0.7000 - val_loss: 1.2057\n",
            "Epoch 29/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7585 - loss: 1.0394 - val_accuracy: 0.7000 - val_loss: 1.1934\n",
            "Epoch 30/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7019 - loss: 1.0557 - val_accuracy: 0.7000 - val_loss: 1.1908\n",
            "Epoch 31/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.6686 - loss: 1.1059 - val_accuracy: 0.7000 - val_loss: 1.1263\n",
            "Epoch 32/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7632 - loss: 1.0459 - val_accuracy: 0.8000 - val_loss: 1.0696\n",
            "Epoch 33/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.6964 - loss: 1.0069 - val_accuracy: 0.9000 - val_loss: 1.0428\n",
            "Epoch 34/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.6908 - loss: 1.0139 - val_accuracy: 0.7000 - val_loss: 1.0289\n",
            "Epoch 35/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7966 - loss: 0.8861 - val_accuracy: 0.7000 - val_loss: 1.0291\n",
            "Epoch 36/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7002 - loss: 0.9882 - val_accuracy: 0.9000 - val_loss: 0.9797\n",
            "Epoch 37/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7893 - loss: 0.8955 - val_accuracy: 0.8000 - val_loss: 0.9457\n",
            "Epoch 38/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.6103 - loss: 1.1171 - val_accuracy: 0.9000 - val_loss: 0.8883\n",
            "Epoch 39/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7502 - loss: 0.8852 - val_accuracy: 0.9000 - val_loss: 0.8903\n",
            "Epoch 40/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8532 - loss: 0.8488 - val_accuracy: 0.9000 - val_loss: 0.9543\n",
            "Epoch 41/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8226 - loss: 0.7378 - val_accuracy: 0.8000 - val_loss: 0.9361\n",
            "Epoch 42/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8793 - loss: 0.7566 - val_accuracy: 0.9000 - val_loss: 0.8595\n",
            "Epoch 43/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7808 - loss: 0.9232 - val_accuracy: 0.9000 - val_loss: 0.8414\n",
            "Epoch 44/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7502 - loss: 0.8226 - val_accuracy: 0.8000 - val_loss: 0.8869\n",
            "Epoch 45/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7410 - loss: 0.8243 - val_accuracy: 0.8000 - val_loss: 0.8934\n",
            "Epoch 46/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7541 - loss: 0.9055 - val_accuracy: 0.9000 - val_loss: 0.7793\n",
            "Epoch 47/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8560 - loss: 0.7069 - val_accuracy: 0.8000 - val_loss: 0.8007\n",
            "Epoch 48/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7652 - loss: 0.7808 - val_accuracy: 0.8000 - val_loss: 0.8803\n",
            "Epoch 49/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8374 - loss: 0.6467 - val_accuracy: 0.8000 - val_loss: 0.8552\n",
            "Epoch 50/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7957 - loss: 0.7597 - val_accuracy: 0.9000 - val_loss: 0.7517\n",
            "Epoch 51/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8385 - loss: 0.7369 - val_accuracy: 0.9000 - val_loss: 0.6778\n",
            "Epoch 52/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8551 - loss: 0.6718 - val_accuracy: 0.9000 - val_loss: 0.6704\n",
            "Epoch 53/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8793 - loss: 0.6698 - val_accuracy: 0.9000 - val_loss: 0.7330\n",
            "Epoch 54/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8635 - loss: 0.6042 - val_accuracy: 0.9000 - val_loss: 0.7391\n",
            "Epoch 55/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8784 - loss: 0.6412 - val_accuracy: 0.9000 - val_loss: 0.7274\n",
            "Epoch 56/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8051 - loss: 0.6153 - val_accuracy: 0.9000 - val_loss: 0.7053\n",
            "Epoch 57/100\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7882 - loss: 0.7215 - val_accuracy: 0.9000 - val_loss: 0.6741\n",
            "Epoch 57: early stopping\n",
            "Restoring model weights from the end of the best epoch: 52.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✨ Best epoch: 52\n",
            "✅ Validation accuracy at best epoch: 0.9000\n"
          ]
        }
      ],
      "source": [
        "# H. 4) TRAIN AND TEST MODEL. SAVE AS model_4.h5\n",
        "num_classes = 7 #angry, disgust, fear, happy, sad, surprise, neutral\n",
        "batch_size = 12\n",
        "epochs = 100\n",
        "\n",
        "def train_3d_cnn(x, y):\n",
        "    model = create_model_3d()\n",
        "\n",
        "    # Early stopping\n",
        "    early_stop = keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(1e-4),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x, y,\n",
        "        validation_split=0.2,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        callbacks=[early_stop]\n",
        "    )\n",
        "\n",
        "    # Early Stop Parameters\n",
        "    best_epoch = np.argmin(history.history['val_loss']) + 1\n",
        "    best_val_acc = history.history['val_accuracy'][best_epoch - 1]\n",
        "    print(f\"✨ Best epoch: {best_epoch}\")\n",
        "    print(f\"✅ Validation accuracy at best epoch: {best_val_acc:.4f}\")\n",
        "\n",
        "    model.save('model_4.h5')\n",
        "\n",
        "train_3d_cnn(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEsXqNUko1bD"
      },
      "source": [
        "5) In your writeup, include:\n",
        "\n",
        "*   A diagram of your final network architecture\n",
        "*   A description of the structure\n",
        "*   The test/train ratio\n",
        "*   The parameters you used (batch size, number of epochs)\n",
        "*   The accuracy and loss for your model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t1OG6-_Qe3y"
      },
      "source": [
        "## Extra Credit\n",
        "###Extra Credit 1 (5 points):\n",
        "Continue to modify your model until you achieve an accuracy >= 90%. Save your model as model_ec1.h5.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5znD9JC1hGj4"
      },
      "outputs": [],
      "source": [
        "# EC. 1) Accuracy >= 90%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csw2E6B8hGvb"
      },
      "source": [
        "\n",
        "###Extra Credit 2 (10 points maximum):\n",
        "Use a different classification technique to classify the data (it does not have to be deep learning). Describe what you built and report how well your classification works. Be sure to include where you found inspiration for the implementation and what additional libraries you used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQ9-8FJ7RAn1"
      },
      "outputs": [],
      "source": [
        "# EC. 2) Different Classification Technique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U61dRxyKQzyT"
      },
      "source": [
        "## Exporting your Colab Notebook for Submission\n",
        "\n",
        "Once you're done implmenting all the parts of the project, you will need to download your Colab Notebook and models to include as part of your submission.\n",
        "\n",
        "1) For each model you saved, download it by right-clicking on it in the files and selecting download.\n",
        "\n",
        "2) To download the notebook, click *download .ipynb* in the file menu.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}