{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMgl8XAQTunasCdIAE3taR/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Justin-Qian/ML_Homework_Collection/blob/main/MoMa_MoE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "5eYE8-Qlmj07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gccXOhL5kleC",
        "outputId": "832738ae-9588-4f08-f878-93715d5c9eba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Examine PyTorch Version\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "\n",
        "# Check CUDA Availability\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "# Choose Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expert"
      ],
      "metadata": {
        "id": "Ax-qlqwRmzZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Expert(nn.Module):\n",
        "    \"\"\"FFN_SwiGLU for each expert\"\"\"\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.w1 = nn.Linear(input_dim, output_dim * 2, bias=False)\n",
        "        self.w2 = nn.Linear(input_dim, output_dim * 2, bias=False)\n",
        "        self.w3 = nn.Linear(output_dim * 2, output_dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = F.linear(x, self.w1.weight)  # Compute Wx (No bias)\n",
        "        x2 = F.linear(x, self.w2.weight)  # Compute Wx (No bias)\n",
        "        hidden = F.silu(x1) * x2  # SwiGLU: SiLU(Gate) âŠ— Linear Transformation\n",
        "        return F.linear(hidden, self.w3.weight)  # Final transformation"
      ],
      "metadata": {
        "id": "VQOtxLvrmDwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Router"
      ],
      "metadata": {
        "id": "hXznYzaem63r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Router(nn.Module):\n",
        "    \"\"\"Expert-Choice (EC) Routing for Mixture of Experts\"\"\"\n",
        "    def __init__(self, input_dim, num_experts, capacity_factor=1.0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim (int): The input feature dimension.\n",
        "            num_experts (int): The number of experts.\n",
        "            capacity_factor (float): The fraction of tokens each expert can process.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_experts = num_experts\n",
        "        self.capacity_factor = capacity_factor\n",
        "        self.WMg = nn.Linear(input_dim, num_experts)  # Project input features to expert selection scores\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape [batch_size, input_dim].\n",
        "            modality_mask (Tensor): Boolean mask indicating which tokens belong to the current modality.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Routing weights of shape [batch_size, num_experts], with unselected tokens set to zero.\n",
        "        \"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # Compute token-to-expert affinity scores\n",
        "        scores = self.WMg(x)  # Shape: [batch_size, num_experts]\n",
        "        routing_weights = torch.sigmoid(scores)  # Shape: [batch_size, num_experts]\n",
        "\n",
        "        # Keep only top-ke tokens for each expert\n",
        "        ke = int(batch_size * (self.capacity_factor / self.num_experts))\n",
        "        top_ke_routing = self.top_k_expert_selection(routing_weights, ke)\n",
        "\n",
        "        return top_ke_routing\n",
        "\n",
        "    def top_k_expert_selection(self, routing_weights, ke):\n",
        "        \"\"\"\n",
        "        Select top-ke tokens for each expert.\n",
        "\n",
        "        Args:\n",
        "            routing_weights (Tensor): Shape [batch_size, num_experts].\n",
        "            ke (int): Number of tokens each expert can process.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Updated routing weights with only top-ke tokens selected per expert.\n",
        "        \"\"\"\n",
        "        batch_size, num_experts = routing_weights.shape\n",
        "        mask = torch.zeros_like(routing_weights)  # Initialize mask with zeros\n",
        "\n",
        "        # Calculate topk and create mask\n",
        "        top_values, top_indices = torch.topk(routing_weights, ke, dim=0)\n",
        "        mask.scatter_(0, top_indices, 1)\n",
        "\n",
        "        # Apply mask to routing weights (only top-ke tokens per expert remain)\n",
        "        return routing_weights * mask\n"
      ],
      "metadata": {
        "id": "HusUehXCmv9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MoE Layer"
      ],
      "metadata": {
        "id": "RAtshg-aPw2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MoE_Layer(nn.Module):\n",
        "    \"\"\"Mixture of Experts (MoE) layer with expert groups.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, num_experts=4):\n",
        "        super().__init__()\n",
        "        self.experts = nn.ModuleList([Expert(input_dim, output_dim) for _ in range(num_experts)])\n",
        "        self.router = Router(input_dim, num_experts)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape [batch_size, input_dim].\n",
        "        Returns:\n",
        "            Tensor: The weighted sum of expert outputs.\n",
        "        \"\"\"\n",
        "        # Compute weights\n",
        "        routing_weights = self.router(x)  # [batch_size, num_experts]\n",
        "        mask = routing_weights != 0       # [batch_size, num_experts]\n",
        "\n",
        "        # Reshape routing_weights to [batch_size, num_experts, 1]\n",
        "        routing_weights = routing_weights.unsqueeze(-1)\n",
        "\n",
        "        # Prepare a list to hold expert outputs, initialized with zeros\n",
        "        batch_size, num_experts = mask.shape\n",
        "        output_dim = self.experts[0](x).shape[-1]\n",
        "        device = x.device\n",
        "        expert_outputs = torch.zeros(batch_size, num_experts, output_dim, device=device)\n",
        "\n",
        "        # Compute expert(x) **only when** routing_weight != 0 (Core!!!This is why MOE is efficient)\n",
        "        for i, expert in enumerate(self.experts):\n",
        "            active = mask[:, i]\n",
        "            if active.any():\n",
        "                expert_out = expert(x[active])  # [num_active, output_dim]\n",
        "                expert_outputs[active, i] = expert_out\n",
        "\n",
        "        # Compute weighted sum of expert outputs\n",
        "        output = torch.sum(routing_weights * expert_outputs, dim=1)  # [batch_size, output_dim]\n",
        "        return output"
      ],
      "metadata": {
        "id": "FNsN_csen0e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Architecture"
      ],
      "metadata": {
        "id": "YduZkGHCnG_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MoMa_Module(nn.Module):\n",
        "    \"\"\"Complete MoMa module with hierarchical routing and modality-aware experts.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, num_text_experts=4, num_image_experts=4):\n",
        "        super().__init__()\n",
        "        # Text MoE\n",
        "        self.text_moe = MoE_Layer(input_dim, output_dim, num_text_experts)\n",
        "        # Image MoE\n",
        "        self.image_moe = MoE_Layer(input_dim, output_dim, num_image_experts)\n",
        "\n",
        "    def forward(self, x, modality_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape [batch_size, input_dim].\n",
        "            modality_mask (Tensor): Boolean mask of shape [batch_size],\n",
        "                                    True for text (\"T\"), False for image (\"I\").\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Output from the MoE layer after expert routing, preserving original order.\n",
        "        \"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # Create masks\n",
        "        text_mask = modality_mask  # True for text tokens\n",
        "        image_mask = ~modality_mask  # False for image tokens\n",
        "\n",
        "        # Process Text Tokens\n",
        "        text_output = torch.zeros(batch_size, x.shape[-1], device=x.device)  # Placeholder\n",
        "        if text_mask.any():\n",
        "            text_x = x[text_mask]\n",
        "            text_output[text_mask] = self.text_moe(text_x)\n",
        "\n",
        "        # Process Image Tokens\n",
        "        image_output = torch.zeros(batch_size, x.shape[-1], device=x.device)  # Placeholder\n",
        "        if image_mask.any():\n",
        "            image_x = x[image_mask]\n",
        "            image_output[image_mask] = self.image_moe(image_x)\n",
        "\n",
        "        # Merge Outputs (preserving original order)\n",
        "        output = torch.zeros(batch_size, x.shape[-1], device=x.device)  # Final output tensor\n",
        "        output[text_mask] = text_output[text_mask]\n",
        "        output[image_mask] = image_output[image_mask]\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "QrMd5v12m85K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test"
      ],
      "metadata": {
        "id": "_whAq9EGnFOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input parameters\n",
        "input_dim = 128  # Transformer hidden dimension\n",
        "output_dim = 128  # Output feature dimension\n",
        "batch_size = 8  # Number of tokens per batch\n",
        "\n",
        "# Generate random input tokens\n",
        "x = torch.randn(batch_size, input_dim).to(device)\n",
        "\n",
        "# Generate a random modality mask (True for text, False for image)\n",
        "modality_mask = torch.randint(0, 2, (batch_size,)).bool().to(device)\n",
        "\n",
        "# Initialize MoMa module\n",
        "moma = MoMa_Module(input_dim, output_dim).to(device)\n",
        "\n",
        "# Forward pass\n",
        "output = moma(x, modality_mask)\n",
        "\n",
        "# Print results\n",
        "print(\"Modality mask:\", modality_mask)\n",
        "print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HycaIh-ZnDrO",
        "outputId": "b0c7de71-d866-48c6-95ed-0b695d018bd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modality mask: tensor([False, False,  True,  True,  True,  True, False,  True],\n",
            "       device='cuda:0')\n",
            "Output shape: torch.Size([8, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(moma)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdnfkYQRxM78",
        "outputId": "7d79858d-a339-4692-d764-b52855e4c412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoMa_Module(\n",
            "  (text_moe): MoE_Layer(\n",
            "    (experts): ModuleList(\n",
            "      (0-3): 4 x Expert(\n",
            "        (w1): Linear(in_features=128, out_features=256, bias=False)\n",
            "        (w2): Linear(in_features=128, out_features=256, bias=False)\n",
            "        (w3): Linear(in_features=256, out_features=128, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (router): Router(\n",
            "      (WMg): Linear(in_features=128, out_features=4, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (image_moe): MoE_Layer(\n",
            "    (experts): ModuleList(\n",
            "      (0-3): 4 x Expert(\n",
            "        (w1): Linear(in_features=128, out_features=256, bias=False)\n",
            "        (w2): Linear(in_features=128, out_features=256, bias=False)\n",
            "        (w3): Linear(in_features=256, out_features=128, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (router): Router(\n",
            "      (WMg): Linear(in_features=128, out_features=4, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    }
  ]
}